>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group 9      |
|-----------------|
| Prabhjodh Randhawa                |   
| Sebastian Silva              |   
| Nitish Varshan Vinotha Karthikeyan               |   
| Thanishque Harshaa                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

This lab introduces us to practical software testing techniques through hands-on examples and interactions with a real system and Jira, a defect tracking tool. The main goal of this lab is to see how different testing approaches, exploratory, manual scripted, and regression testing, are applied in a real-world setting. Prior to this lab, our knowledge of the different testing approaches was mainly limited to theory. We knew that exploratory testing was a lot more informal and relied on the tester’s intuition, experience, and curiosity. Similarly, our familiarity with manual scripted testing ended with knowing that it was the more formal approach and consisted of predefined test cases. Due to this, we had not previously thought about comparing the two approaches against each other to see their effectiveness in a realistic scenario. Through this lab, we were able to apply these techniques and methods to an ATM simulation and better understand the strengths and limitations of each testing approach.

# High-level description of the exploratory testing plan

Our exploratory testing plan was mainly focused on a free flow, curiosity first, informal system. As stated in the introduction, our knowledge of exploratory testing mainly was that in theory it was based on the tester's intuition, experience, and curiosity. We decided as a team this concept would be the most effective in our testing. Our group mainly followed their own process when testing, trying to discover any bugs they could find. Although we had freedom as our main priority, we collectively decided to prioritize realistic user workflow instead of testing every function exhaustively. Some of these targeted areas include card insertion and PIN validation, withdrawals, deposits, transfers, and transaction cancellation. We intentionally explored exceptional and boundary cases, attempting to discover the most profound bugs in the system. Defects were recorded immediately upon discovery to ensure reproduction steps and system state were accurately captured.

# Comparison of exploratory and manual functional testing

Exploratory and manual testing both have their disadvantages and their advantages, they offer distinct perspectives and trade-offs. Exploratory testing was very efficient in discovering usability issues and other unexpected defects. Due to allowing the tester to follow their intuition and curiosity, testers were able to adapt quickly and investigate suspicious behaviour in much greater depth than manual testing would allow for. But this higher degree of flexibility came with its own trade-offs in less coverage and lower repeatability, making consistency an issue through all the tests. Manual testing, on the other hand, did not have these disadvantages, being structured and repeatable due to the predefined test cases. Ensuring great and clear coverage of the system, making it simpler to see if the program was meeting specifications. However, due to it being strict in its predefined test cases, it was mostly impossible to discover defects outside this scope. Overall, both testing methods have their positives and negatives. Exploratory was more efficient in early defect discovery and defect discovery outside the scope of the test cases, while manual excels at clear coverage, validation, and regression testing. The combination of both of these methods allows for defect discovery at an unprecedented level, more than either approach could do by itself.

-   Note that you need to submit a report generated by your defect tracking
    system, containing all defects recorded in the system.

# Notes and discussion of the peer reviews of defect reports

Peer review of defect reports helped improve clarity, consistency, and reproducibility across the defect list. Reviewers identified areas where reproduction steps or expected results were unclear and provided feedback to better align reports with documented system requirements. Descriptions were altered to provide a better overview of each defect/bug. Peer review also helped identify overlapping reports and encouraged a consistent structure, resulting in a more readable and professional defect backlog. 

# How the pair testing was managed and team work/effort was divided 

Our group discussed the work load through various voice-calls/recurring meetings that were agreed upon prior through physical meetings and modes of communciations such as e-mails and messages. From there we had split up the work and ensured that we would meet the deadlines we had set for each other so that everything could be reviewed and verified. Nitish and Prabhjodh were assigned the task of handling the manual scripted testing, while Sebastian and Thanishque were assigned the task of exploratory testing. Further, Nitish and Thanishque comopleted the regression testing through a video call with the help of each other. Afterwards, we regrouped to have an overview of everything logged up to the day of the Demo, and each person was required to memorize the structure/status of 2 bugs of their choosing in order to present on Demo day. The rest of the lab report was completed collectively with the use of a voice-chat to express our inputs better, with Nitish and Thanishque completing the logging of bugs in Jira and Prabhjodh and Sebastian working on the report. Hence, our work was split very evenly and fairly and let us complete our respective tasks without a bad taste in our mouths.

# Difficulties encountered, challenges overcome, and lessons learned

One of the main challenges encountered during this lab was the presence of ambiguous test cases, which partially hindered the execution of manual scripted testing. In several situations, it was unclear whether unexpected system behavior indicated a genuine defect or simply resulted from an incorrect interpretation of the test case instructions. This ambiguity led to additional time being spent verifying steps and confirming expected outcomes.

During exploratory testing, a key challenge was determining where to begin and how to approach testing in an organized manner. Because the system appeared simple on the surface but allowed many possible interaction paths, it was initially difficult to decide which scenarios to prioritize. Developing a structured exploratory plan helped mitigate this issue and allowed testing to proceed more efficiently.

Another significant challenge was identifying the root causes of inconsistent system behavior. In some cases, performing the same action produced different results depending on prior system state or input history. Certain defects only appeared under very specific conditions, requiring repeated testing to reliably reproduce them. Additionally, distinguishing between issues caused by invalid user input and those caused by internal system logic required careful observation and detailed documentation.

This experience highlighted the importance of writing precise reproduction steps and maintaining clear defect reports. In real-world software development scenarios, such as video game development or financial systems, developers must first understand how a defect occurs before it can be fixed. Reproducing defects consistently is often difficult because it is impossible to anticipate every possible use case during development.

Overall, this lab demonstrated that even relatively simple systems can contain serious defects when tested beyond expected usage scenarios. It emphasized the importance of thorough input validation, defensive programming, and careful state management, especially in financial applications. The lab also reinforced that exploratory testing is effective for uncovering unexpected issues, while manual functional testing is essential for verifying compliance with specified requirements. Together, these testing approaches form a robust and complementary testing strategy.


# Comments/feedback on the lab and lab document itself

Overall, the lab was a useful and practical introduction to software testing and defect tracking. The ATM system provided a realistic environment for discovering both functional and edge-case defects, and the testing phases helped clarify the differences between exploratory, manual, and regression testing. One area for improvement would be providing clearer guidance on expected defect report detail and examples of well-written reports. Despite this, the lab was effective in reinforcing key testing concepts and provided valuable hands-on experience.
